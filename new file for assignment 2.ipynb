{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad4010c9",
   "metadata": {},
   "source": [
    "# Assignment 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc93c8b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: selenium in c:\\users\\lavina\\appdata\\roaming\\python\\python39\\site-packages (4.7.2)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\lavina\\appdata\\roaming\\python\\python39\\site-packages (from selenium) (0.22.0)\n",
      "Requirement already satisfied: urllib3[socks]~=1.26 in c:\\programdata\\anaconda3\\lib\\site-packages (from selenium) (1.26.11)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\programdata\\anaconda3\\lib\\site-packages (from selenium) (2022.9.14)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\lavina\\appdata\\roaming\\python\\python39\\site-packages (from selenium) (0.9.2)\n",
      "Requirement already satisfied: sortedcontainers in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.0rc9 in c:\\users\\lavina\\appdata\\roaming\\python\\python39\\site-packages (from trio~=0.17->selenium) (1.0.4)\n",
      "Requirement already satisfied: async-generator>=1.9 in c:\\users\\lavina\\appdata\\roaming\\python\\python39\\site-packages (from trio~=0.17->selenium) (1.10)\n",
      "Requirement already satisfied: outcome in c:\\users\\lavina\\appdata\\roaming\\python\\python39\\site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: sniffio in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: attrs>=19.2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (21.4.0)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.15.1)\n",
      "Requirement already satisfied: idna in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (3.3)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\lavina\\appdata\\roaming\\python\\python39\\site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in c:\\programdata\\anaconda3\\lib\\site-packages (from urllib3[socks]~=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: pycparser in c:\\programdata\\anaconda3\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.21)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\lavina\\appdata\\roaming\\python\\python39\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n"
     ]
    }
   ],
   "source": [
    "#install and import required libraries\n",
    "!pip install selenium\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import StaleElementReferenceException, NoSuchElementException\n",
    "from selenium.webdriver.common.by import By\n",
    "import time \n",
    "from time import sleep\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983dc12e",
   "metadata": {},
   "source": [
    "1.scraping data for data analyst post for bangalore location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60b311e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 0 0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Job</th>\n",
       "      <th>Location</th>\n",
       "      <th>Company</th>\n",
       "      <th>Experience</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Job, Location, Company, Experience]\n",
       "Index: []"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# connecting to the driver\n",
    "driver = webdriver.Chrome(r\"chromedriver.exe\")\n",
    "#opening the required page(naukri.com) on automated chrome browser\n",
    "driver.get(\"https://www.naukri.com/\")\n",
    "\n",
    "#enter designation as required in question\n",
    "designation=driver.find_element(By.CLASS_NAME,\"suggestor-input \")\n",
    "designation.send_keys('Data Analyst')\n",
    "#enter location as required in question\n",
    "location=driver.find_element(By.XPATH,\"/html/body/div[1]/div[6]/div/div/div[5]/div/div/div/input\")\n",
    "location.send_keys('Bangalore')\n",
    "#for clicking search button\n",
    "search=driver.find_element(By.CLASS_NAME,\"qsbSubmit\")\n",
    "search.click()\n",
    "\n",
    "#making blank list for required data given in questions\n",
    "job_title = []\n",
    "job_location = []\n",
    "company_name = []\n",
    "experience_required = []\n",
    "\n",
    "#scraping job title from given page\n",
    "title_tags = driver.find_elements(By.XPATH,'//a[@class=\"title fw500 ellipsis\"]')\n",
    "for i in title_tags[0:10]:\n",
    "    title = i.text\n",
    "    job_title.append(title)\n",
    "\n",
    "#scraping location from given page\n",
    "location_tags = driver.find_elements(By.XPATH,'//li[@class=\"fleft grey-text br2 placeHolderLi location\"]')\n",
    "for i in location_tags[0:10]:\n",
    "    location = i.text\n",
    "    job_location.append(location)\n",
    "\n",
    "#scraping company name from given page\n",
    "company_tags = driver.find_elements(By.XPATH,'//a[@class=\"subTitle ellipsis fleft\"]')\n",
    "for i in company_tags[0:10]:\n",
    "    company = i.text\n",
    "    company_name.append(company)\n",
    "\n",
    "#scraping experience from given page\n",
    "experience_tags = driver.find_elements(By.XPATH,'//li[@class=\"fleft grey-text br2 placeHolderLi experience\"]//span')\n",
    "for i in experience_tags[0:10]:\n",
    "    experience = i.text\n",
    "    experience_required.append(experience)\n",
    "job_title \n",
    "job_location\n",
    "company_name\n",
    "experience_required   \n",
    "\n",
    " # printing length\n",
    "print(len(job_title),len(job_location),len(company_name ),len(experience_required ))\n",
    "\n",
    "\n",
    "# making dataframe\n",
    "df= pd.DataFrame({'Job':job_title,'Location':job_location,'Company':company_name,'Experience':experience_required} )\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa74e13",
   "metadata": {},
   "source": [
    "2.scraping a data for data scientist post in banglore location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b75c0dc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 10 10 10\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Job</th>\n",
       "      <th>Location</th>\n",
       "      <th>Company</th>\n",
       "      <th>Experience</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Analystics &amp; Modeling Specialist</td>\n",
       "      <td>Bangalore/Bengaluru, Kolkata, Mumbai, Hyderaba...</td>\n",
       "      <td>Accenture</td>\n",
       "      <td>6-8 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Bangalore/Bengaluru, New Delhi, Hyderabad/Secu...</td>\n",
       "      <td>Tata Nexarc</td>\n",
       "      <td>4-8 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Bangalore/Bengaluru, Nagpur, Pune</td>\n",
       "      <td>Tech Mahindra</td>\n",
       "      <td>5-8 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Data &amp; Analytics Lead, Geo Analytics - GAMMA</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Boston Consulting Group</td>\n",
       "      <td>7-10 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Weather and Climate Data Scientist</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Shell Pvt Ltd</td>\n",
       "      <td>5-12 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Hybrid - Bangalore/Bengaluru, Noida, Kolkata, ...</td>\n",
       "      <td>Mindtree</td>\n",
       "      <td>5-10 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Data Scientist - II</td>\n",
       "      <td>Bangalore/Bengaluru, India, Mumbai (All Areas)</td>\n",
       "      <td>Bizongo</td>\n",
       "      <td>3-6 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Senior Data Scientist</td>\n",
       "      <td>Bangalore/Bengaluru, Mumbai</td>\n",
       "      <td>Baker Hughes</td>\n",
       "      <td>6-8 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Manager-Data Science</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>AMERICAN EXPRESS</td>\n",
       "      <td>3-4 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ACN - Applied Intelligence - Data Scientist - 09</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Accenture</td>\n",
       "      <td>2-6 Yrs</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Job  \\\n",
       "0                  Analystics & Modeling Specialist   \n",
       "1                                    Data Scientist   \n",
       "2                                    Data Scientist   \n",
       "3      Data & Analytics Lead, Geo Analytics - GAMMA   \n",
       "4                Weather and Climate Data Scientist   \n",
       "5                                    Data Scientist   \n",
       "6                               Data Scientist - II   \n",
       "7                             Senior Data Scientist   \n",
       "8                              Manager-Data Science   \n",
       "9  ACN - Applied Intelligence - Data Scientist - 09   \n",
       "\n",
       "                                            Location                  Company  \\\n",
       "0  Bangalore/Bengaluru, Kolkata, Mumbai, Hyderaba...                Accenture   \n",
       "1  Bangalore/Bengaluru, New Delhi, Hyderabad/Secu...              Tata Nexarc   \n",
       "2                  Bangalore/Bengaluru, Nagpur, Pune            Tech Mahindra   \n",
       "3                                Bangalore/Bengaluru  Boston Consulting Group   \n",
       "4                                Bangalore/Bengaluru            Shell Pvt Ltd   \n",
       "5  Hybrid - Bangalore/Bengaluru, Noida, Kolkata, ...                 Mindtree   \n",
       "6     Bangalore/Bengaluru, India, Mumbai (All Areas)                  Bizongo   \n",
       "7                        Bangalore/Bengaluru, Mumbai             Baker Hughes   \n",
       "8                                Bangalore/Bengaluru         AMERICAN EXPRESS   \n",
       "9                                Bangalore/Bengaluru                Accenture   \n",
       "\n",
       "  Experience  \n",
       "0    6-8 Yrs  \n",
       "1    4-8 Yrs  \n",
       "2    5-8 Yrs  \n",
       "3   7-10 Yrs  \n",
       "4   5-12 Yrs  \n",
       "5   5-10 Yrs  \n",
       "6    3-6 Yrs  \n",
       "7    6-8 Yrs  \n",
       "8    3-4 Yrs  \n",
       "9    2-6 Yrs  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# connecting to the driver\n",
    "driver = webdriver.Chrome(r\"chromedriver.exe\")\n",
    "#opening the required page(naukri.com) on automated chrome browser\n",
    "driver.get(\"https://www.naukri.com/\")\n",
    "#enter designation as required in question\n",
    "designation=driver.find_element(By.CLASS_NAME,\"suggestor-input \")\n",
    "designation.send_keys('Data scientist')\n",
    "#enter location as required in question\n",
    "location=driver.find_element(By.XPATH,\"/html/body/div[1]/div[6]/div/div/div[5]/div/div/div/input\")\n",
    "location.send_keys('Bangalore')\n",
    "#for clicking search button\n",
    "search=driver.find_element(By.CLASS_NAME,\"qsbSubmit\")\n",
    "search.click()\n",
    "#making blank list for required data given in questions\n",
    "job_title = []\n",
    "job_location = []\n",
    "company_name = []\n",
    "experience_required = []\n",
    "\n",
    "#scraping job title from given page\n",
    "title_tags = driver.find_elements(By.XPATH,'//a[@class=\"title fw500 ellipsis\"]')\n",
    "for i in title_tags[0:10]:\n",
    "    title = i.text\n",
    "    job_title.append(title)\n",
    "\n",
    "#scraping location from given page\n",
    "location_tags = driver.find_elements(By.XPATH,'//li[@class=\"fleft grey-text br2 placeHolderLi location\"]')\n",
    "for i in location_tags[0:10]:\n",
    "    location = i.text\n",
    "    job_location.append(location)\n",
    "\n",
    "#scraping company name from given page\n",
    "company_tags = driver.find_elements(By.XPATH,'//a[@class=\"subTitle ellipsis fleft\"]')\n",
    "for i in company_tags[0:10]:\n",
    "    company = i.text\n",
    "    company_name.append(company)\n",
    "\n",
    "#scraping experience from given page\n",
    "experience_tags = driver.find_elements(By.XPATH,'//li[@class=\"fleft grey-text br2 placeHolderLi experience\"]//span')\n",
    "for i in experience_tags[0:10]:\n",
    "    experience = i.text\n",
    "    experience_required.append(experience)\n",
    "    \n",
    " # printing length\n",
    "print(len(job_title),len(job_location),len(company_name ),len(experience_required ))\n",
    "\n",
    "\n",
    "# making dataframe\n",
    "df= pd.DataFrame({'Job':job_title,'Location':job_location,'Company':company_name,'Experience':experience_required} )\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9ee216",
   "metadata": {},
   "source": [
    "3.scraping a data for data scientist post in bangalore location with filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c3ab8be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 0 0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Job Title</th>\n",
       "      <th>Location</th>\n",
       "      <th>Company</th>\n",
       "      <th>Experience</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Job Title, Location, Company, Experience]\n",
       "Index: []"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# connecting to the driver\n",
    "driver = webdriver.Chrome(r\"chromedriver.exe\")\n",
    "#opening the required page(naukri.com) on automated chrome browser\n",
    "driver.get(\"https://www.naukri.com/\")\n",
    "#entering designation as required in question and click on search button\n",
    "designation = driver.find_element(By.CLASS_NAME,\"suggestor-input\")\n",
    "designation.send_keys('Data Scientist')\n",
    "\n",
    "search = driver.find_element(By.CLASS_NAME,\"qsbSubmit\")\n",
    "search.click()\n",
    "job_title = []\n",
    "job_location = []\n",
    "company_name = []\n",
    "experience_required = []\n",
    "\n",
    "#scraping job title from given page\n",
    "title_tags = driver.find_elements(By.XPATH,'//a[@class=\"title fw500 ellipsis\"]')\n",
    "for i in title_tags[0:10]:\n",
    "    title = i.text\n",
    "    job_title.append(title)\n",
    "    \n",
    "#scraping location from given page\n",
    "location_tags = driver.find_elements(By.XPATH,'//li[@class=\"fleft grey-text br2 placeHolderLi location\"]')\n",
    "for i in location_tags[0:10]:\n",
    "    location = i.text\n",
    "    job_location.append(location)\n",
    "    \n",
    "#scraping company name from given page\n",
    "company_tags = driver.find_elements(By.XPATH,'//a[@class=\"subTitle ellipsis fleft\"]')\n",
    "for i in company_tags[0:10]:\n",
    "    company = i.text\n",
    "    company_name.append(company)\n",
    "    \n",
    "#scraping experience from given page\n",
    "experience_tags = driver.find_elements(By.XPATH,'//li[@class=\"fleft grey-text br2 placeHolderLi experience\"]//span')\n",
    "\n",
    "for i in experience_tags[0:10]:\n",
    "    experience = i.text\n",
    "    experience_required.append(experience)\n",
    "    \n",
    "#printing length    \n",
    "print(len(job_title),len(job_location),len(company_name),len(experience_required))\n",
    "\n",
    "# making dataframe\n",
    "df = pd.DataFrame({'Job Title':job_title,'Location':job_location,'Company':company_name,'Experience':experience_required})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa90631",
   "metadata": {},
   "source": [
    "4.scrap data for first 100 sunglasses from flipkart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a2de19e6",
   "metadata": {},
   "outputs": [
    {
     "ename": "ElementClickInterceptedException",
     "evalue": "Message: element click intercepted: Element <path class=\"_34RNph\" d=\"m11.618 9.897l4.225 4.212c.092.092.101.232.02.313l-1.465 1.46c-.081.081-.221.072-.314-.02l-4.216-4.203\"></path> is not clickable at point (558, 32). Other element would receive the click: <div class=\"_2hriZF _2rbIyg\" tabindex=\"-1\">...</div>\n  (Session info: chrome=108.0.5359.99)\nStacktrace:\nBacktrace:\n\t(No symbol) [0x0041F243]\n\t(No symbol) [0x003A7FD1]\n\t(No symbol) [0x0029D04D]\n\t(No symbol) [0x002D28B9]\n\t(No symbol) [0x002D08CC]\n\t(No symbol) [0x002CE4CB]\n\t(No symbol) [0x002CD164]\n\t(No symbol) [0x002C32A6]\n\t(No symbol) [0x002E858C]\n\t(No symbol) [0x002C2BFF]\n\t(No symbol) [0x002E8804]\n\t(No symbol) [0x002FC9EB]\n\t(No symbol) [0x002E8386]\n\t(No symbol) [0x002C163C]\n\t(No symbol) [0x002C269D]\n\tGetHandleVerifier [0x006B9A22+2655074]\n\tGetHandleVerifier [0x006ACA24+2601828]\n\tGetHandleVerifier [0x004C8C0A+619850]\n\tGetHandleVerifier [0x004C7830+614768]\n\t(No symbol) [0x003B05FC]\n\t(No symbol) [0x003B5968]\n\t(No symbol) [0x003B5A55]\n\t(No symbol) [0x003C051B]\n\tBaseThreadInitThunk [0x75AA6939+25]\n\tRtlGetFullPathName_UEx [0x775F8FD2+1218]\n\tRtlGetFullPathName_UEx [0x775F8F9D+1165]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mElementClickInterceptedException\u001b[0m          Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_28172\\2556394435.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;31m#for clicking search button\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0msearch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdriver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_element\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCLASS_NAME\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"_34RNph\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0msearch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclick\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0mbrand_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\selenium\\webdriver\\remote\\webelement.py\u001b[0m in \u001b[0;36mclick\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     91\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclick\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m         \u001b[1;34m\"\"\"Clicks the element.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 93\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_execute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCommand\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCLICK_ELEMENT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0msubmit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\selenium\\webdriver\\remote\\webelement.py\u001b[0m in \u001b[0;36m_execute\u001b[1;34m(self, command, params)\u001b[0m\n\u001b[0;32m    408\u001b[0m             \u001b[0mparams\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    409\u001b[0m         \u001b[0mparams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"id\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_id\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 410\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_parent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    411\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    412\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfind_element\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mby\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mBy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mID\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mWebElement\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\selenium\\webdriver\\remote\\webdriver.py\u001b[0m in \u001b[0;36mexecute\u001b[1;34m(self, driver_command, params)\u001b[0m\n\u001b[0;32m    442\u001b[0m         \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcommand_executor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdriver_command\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    443\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 444\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merror_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheck_response\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    445\u001b[0m             \u001b[0mresponse\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"value\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_unwrap_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"value\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    446\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\selenium\\webdriver\\remote\\errorhandler.py\u001b[0m in \u001b[0;36mcheck_response\u001b[1;34m(self, response)\u001b[0m\n\u001b[0;32m    247\u001b[0m                 \u001b[0malert_text\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"alert\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"text\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    248\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mexception_class\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscreen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstacktrace\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malert_text\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[call-arg]  # mypy is not smart enough here\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 249\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mexception_class\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscreen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstacktrace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mElementClickInterceptedException\u001b[0m: Message: element click intercepted: Element <path class=\"_34RNph\" d=\"m11.618 9.897l4.225 4.212c.092.092.101.232.02.313l-1.465 1.46c-.081.081-.221.072-.314-.02l-4.216-4.203\"></path> is not clickable at point (558, 32). Other element would receive the click: <div class=\"_2hriZF _2rbIyg\" tabindex=\"-1\">...</div>\n  (Session info: chrome=108.0.5359.99)\nStacktrace:\nBacktrace:\n\t(No symbol) [0x0041F243]\n\t(No symbol) [0x003A7FD1]\n\t(No symbol) [0x0029D04D]\n\t(No symbol) [0x002D28B9]\n\t(No symbol) [0x002D08CC]\n\t(No symbol) [0x002CE4CB]\n\t(No symbol) [0x002CD164]\n\t(No symbol) [0x002C32A6]\n\t(No symbol) [0x002E858C]\n\t(No symbol) [0x002C2BFF]\n\t(No symbol) [0x002E8804]\n\t(No symbol) [0x002FC9EB]\n\t(No symbol) [0x002E8386]\n\t(No symbol) [0x002C163C]\n\t(No symbol) [0x002C269D]\n\tGetHandleVerifier [0x006B9A22+2655074]\n\tGetHandleVerifier [0x006ACA24+2601828]\n\tGetHandleVerifier [0x004C8C0A+619850]\n\tGetHandleVerifier [0x004C7830+614768]\n\t(No symbol) [0x003B05FC]\n\t(No symbol) [0x003B5968]\n\t(No symbol) [0x003B5A55]\n\t(No symbol) [0x003C051B]\n\tBaseThreadInitThunk [0x75AA6939+25]\n\tRtlGetFullPathName_UEx [0x775F8FD2+1218]\n\tRtlGetFullPathName_UEx [0x775F8F9D+1165]\n"
     ]
    }
   ],
   "source": [
    "# connecting to the driver\n",
    "driver = webdriver.Chrome(r\"chromedriver.exe\")\n",
    "#opening the required page(naukri.com) on automated chrome browser\n",
    "driver.get(\"https://www.flipkart.com/\")\n",
    "#enter product as required in question\n",
    "product=driver.find_element(By.CLASS_NAME,\"_3704LK \")\n",
    "product.send_keys('Sunglasses')\n",
    "\n",
    "#for clicking search button\n",
    "search=driver.find_element(By.CLASS_NAME,\"_34RNph\")\n",
    "search.click()\n",
    "\n",
    "brand_name=[]\n",
    "product_description =[]\n",
    "price_of_product = []\n",
    "start = 0\n",
    "end = 3\n",
    "for pages in range(start,end):\n",
    "    brand = driver.find_elements(By.XPATH,'//div[@class=\"_2WkVRV\"]')\n",
    "    product = driver.find_elements(By.XPATH,'//a[@class=\"IRpwTa\"]')\n",
    "    price = driver.find_elements(By.XPATH,'//a[@class=\"_3bPFwb\"]')\n",
    "    for i in brand:\n",
    "        brand_name.append(i.text)\n",
    "    for i in product:\n",
    "        product_description.append(i.text)\n",
    "    for i in price:\n",
    "        price_of_product.append(i.text)\n",
    "    next_button = driver.find_element(By.XPATH,'//a[@class=\"_1LKTO3\"]')    \n",
    "    next_button.click()\n",
    "    time.sleep(3)\n",
    "    \n",
    "Brand=brand_name0[0:100]\n",
    "Product=product_description[0:100]\n",
    "Price=price_of_product[0:100]\n",
    "#print(len(Brand),len(Product),len(Price))\n",
    "\n",
    "#making dataframe\n",
    "df=pd.DataFrame({'Brand':Brand,'Product':Product,'Price':Price})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb4e457b",
   "metadata": {},
   "source": [
    "5.100 review data from flipkart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12348f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# connecting to the driver\n",
    "driver = webdriver.Chrome(r\"chromedriver.exe\")\n",
    "#opening the required page on automated chrome browser\n",
    "driver.get(\"https://www.flipkart.com/apple-iphone-11-black-64-gb/p/itm4e5041ba101fd?pid=MOBFWQ6BXGJCEYNY&lid=LSTMOBFWQ6BXGJCEYNYZXSHRJ&marketplace=FLIPKART&q=apple+iphone11+black+64+gb&store=tyy%2F4io&spotlightTagId=BestsellerId_tyy%2F4io&srno=s_1_1&otracker=search&otracker1=search&fm=Search&iid=76fc6167-eca7-4945-959e-70e6016e55e4.MOBFWQ6BXGJCEYNY.SEARCH&ppt=sp&ppn=sp&ssid=9fere6h3rk0000001670695888871&qH=faf1208381afa96c\")\n",
    "\n",
    "Rating_of_phone = []\n",
    "Review_summary = []\n",
    "Full_review = []\n",
    "start = 0\n",
    "end = 10\n",
    "for pages in range(start,end):\n",
    "    #scraping rating from given page\n",
    "    rating_tags = driver.find_elements(By.XPATH,'//div[@class=\"_3LWZlK _1BLPMq\"]')\n",
    "    for i in rating_tags:\n",
    "        rating = i.text\n",
    "        Rating_of_phone.append(rating)\n",
    "    \n",
    "    #scraping review from given page\n",
    "    review_tags = driver.find_elements(By.XPATH,'//p[@class=\"_2-N8zT\"]')\n",
    "    for i in review_tags[0:10]:\n",
    "        review = i.text\n",
    "        Review_summary.append(review)\n",
    "    \n",
    "    #scraping full review from given page\n",
    "    full_tags = driver.find_elements(By.XPATH,'//div[@class=\"t-ZTKy\"]')\n",
    "    for i in full_tags[0:10]:\n",
    "        full = i.text\n",
    "        Full_review.append(full)\n",
    "    next_button = driver.find_element(By.XPATH,'//a[@class=\"_1LKTO3\"]')    \n",
    "    next_button.click()\n",
    "    time.sleep(3)\n",
    "\n",
    "#printing length    \n",
    "print(len(Rating_of_phone),len(Review_summary),len(Full_review))\n",
    "\n",
    "# making dataframe\n",
    "df = pd.DataFrame({'Rating_of_phone':Rating_of_phone,'Review_summary':Review_summary,'Full_review':Full_review})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa959240",
   "metadata": {},
   "source": [
    "6.scrap a data for 100 sneakers from flipkart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a848f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# connecting to the driver\n",
    "driver = webdriver.Chrome(r\"chromedriver.exe\")\n",
    "#opening the required page on automated chrome browser\n",
    "driver.get(\"https://www.flipkart.com/\")\n",
    "#enter product as required in question\n",
    "product=driver.find_element(By.CLASS_NAME,\"_3704LK \")\n",
    "product.send_keys('Sneaker')\n",
    "\n",
    "#for clicking search button\n",
    "search=driver.find_element(By.CLASS_NAME,\"_34RNph\")\n",
    "search.click()\n",
    "# creating blank list to get result\n",
    "brand_name=[]\n",
    "product_description =[]\n",
    "price_of_product = []\n",
    "start = 0    \n",
    "end = 3    # defining starting and ending range of webpage\n",
    "\n",
    "# scraping brand,product description and price from webpage\n",
    "for pages in range(start,end):\n",
    "    brand = driver.find_elements(By.ID,'twotabsearchtextbox')\n",
    "    product = driver.find_elements(By.XPATH,'//a[@class=\"IRpwTa\"]')\n",
    "    price = driver.find_elements(By.XPATH,'//a[@class=\"_3bPFwb\"]')\n",
    "    for i in brand:\n",
    "        brand_name.append(i.text)\n",
    "    for i in product:\n",
    "        product_description.append(i.text)\n",
    "    for i in price:\n",
    "        price_of_product.append(i.text)\n",
    "    next_button = driver.find_element(By.XPATH,'//a[@class=\"_1LKTO3\"]')    \n",
    "    next_button.click()\n",
    "    time.sleep(3)\n",
    "Brand=brand_name[0:100]\n",
    "Description=product_description[0:100]\n",
    "Price=price_of_product[0:100]  \n",
    "\n",
    "#print(len(Brand),len(Description),len(Price)).....for printing and checking the length \n",
    "\n",
    "# making dataframe\n",
    "df = pd.DataFrame({'Brand Name':Brand,'Product Description':Description,'Price':Price})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb6363f1",
   "metadata": {},
   "source": [
    "7.scraping data for laptop from amazon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa5bd7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# connecting to the driver\n",
    "driver = webdriver.Chrome(r\"chromedriver.exe\")\n",
    "#opening the required page on automated chrome browser\n",
    "driver.get(\"https://www.amazon.in/\")\n",
    "\n",
    "#entering product as required in question and click on search button\n",
    "product = driver.find_element(By.ID,'twotabsearchtextbox')\n",
    "product.send_keys('Laptop')\n",
    "search = driver.find_element(By.XPATH,'//div[@class=\"nav-search-submit nav-sprite\"]')\n",
    "search.click()\n",
    "\n",
    "#creating a blank list \n",
    "laptop_title = []\n",
    "laptop_rating = []\n",
    "laptop_price = []\n",
    "\n",
    "\n",
    "#scraping laptop title from given page\n",
    "title_tags = driver.find_elements(By.XPATH,'//span[@class=\"a-size-medium a-color-base a-text-normal\"]')\n",
    "for i in title_tags[0:10]:\n",
    "    title = i.text\n",
    "    laptop_title.append(title)\n",
    "    \n",
    "#scraping rating from given page \n",
    "rating_tags = driver.find_elements(By.XPATH,'//span[@class=\"a-size-base\"]')\n",
    "for i in rating_tags[0:10]:\n",
    "    rating = i.text\n",
    "    laptop_rating.append(rating)\n",
    "    \n",
    "#scraping price from given page\n",
    "price_tags = driver.find_elements(By.XPATH,'//span[@class=\"a-price-whole\"]')\n",
    "for i in price_tags[0:10]:\n",
    "    price = i.text\n",
    "    laptop_price.append(price)\n",
    "    \n",
    "\n",
    "    \n",
    "#printing length    \n",
    "print(len(laptop_title),len(laptop_rating),len(laptop_price))\n",
    "\n",
    "# making dataframe\n",
    "df = pd.DataFrame({'laptop_title':laptop_title,'laptop_rating':laptop_rating,'laptop_price':laptop_price})\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a5a121",
   "metadata": {},
   "source": [
    "8.scrape data for top 1000 Quaotes of all time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "23aeac26",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'click'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_28172\\358657197.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[0mnext_button\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdriver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_elements\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mXPATH\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'//li[@class=\"next\"][1]'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m     \u001b[0mnext_button\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclick\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'click'"
     ]
    }
   ],
   "source": [
    "# connecting to the driver\n",
    "driver = webdriver.Chrome(r\"chromedriver.exe\")\n",
    "#opening the required page on automated chrome browser\n",
    "driver.get(\"https://www.azquotes.com/\")\n",
    "\n",
    "# creating blank list to get result\n",
    "Quotes=[]\n",
    "Author =[]\n",
    "Type_of_quotes = []\n",
    "\n",
    "# defining starting and ending range of webpage\n",
    "start = 0    \n",
    "end = 10    \n",
    "\n",
    "# scraping quote, author, type of quote from webpage\n",
    "for pages in range(start,end):\n",
    "    quote_page = driver.find_elements(By.XPATH,'//a[@class=\"title\"]')\n",
    "    author_page = driver.find_elements(By.XPATH,'//div[@class=\"author\"]')\n",
    "    type_page = driver.find_elements(By.XPATH,'//div[@class=\"tags\"]')\n",
    "    \n",
    "    for i in quote_page:\n",
    "        Quotes.append(i.text)\n",
    "    for i in author_page:\n",
    "        Author.append(i.text)\n",
    "    for i in  type_page:\n",
    "        Type_of_quotes.append(i.text)\n",
    "        \n",
    "    next_button = driver.find_elements(By.XPATH,'//li[@class=\"next\"][1]')    \n",
    "    next_button.click()\n",
    "    \n",
    "\n",
    "\n",
    "print(len(Quotes),len(Description),len(Price))   #.....for printing and checking the length \n",
    "\n",
    "# making dataframe\n",
    "df = pd.DataFrame({'Quotes':Quotes,'Author':Author,'Type of quotes':Type_of_quotes})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e492cc04",
   "metadata": {},
   "source": [
    "9.display the list of former prime minister of india"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a3020dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# connecting to the driver\n",
    "driver = webdriver.Chrome(r\"chromedriver.exe\")\n",
    "#opening the required page on automated chrome browser\n",
    "driver.get(\"https://www.jagranjosh.com/\")\n",
    "#scraping table from given page\n",
    "table_tags = driver.find_elements(By.XPATH,'//div[@class=\"table-box\"][1]')\n",
    "table_tags"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855c6b7d",
   "metadata": {},
   "source": [
    "10.list of most 50 expensive car in the world"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e23926e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Car_name</th>\n",
       "      <th>Car_description</th>\n",
       "      <th>Car_price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Car_name, Car_description, Car_price]\n",
       "Index: []"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# connecting to the driver\n",
    "driver = webdriver.Chrome(r\"chromedriver.exe\")\n",
    "#opening the required page on automated chrome browser\n",
    "driver.get(\"https://www.motor1.com/\")\n",
    "\n",
    "#creating a blank list \n",
    "Car_name = []\n",
    "Car_description = []\n",
    "Car_price = []\n",
    "\n",
    "\n",
    "#scraping car name from given page\n",
    "name_tags = driver.find_elements(By.XPATH,'//h3[@class=\"subheader\"]')\n",
    "for i in name_tags[0:10]:\n",
    "    name = i.text\n",
    "    Car_name.append(name)\n",
    "    \n",
    "#scraping car description from given page \n",
    "description_tags = driver.find_elements(By.XPATH,'//span[@class=\"a-size-base\"]')\n",
    "for i in description_tags[0:10]:\n",
    "    description = i.text\n",
    "    Car_description.append(description)\n",
    "    \n",
    "#scraping price from given page\n",
    "price_tags = driver.find_elements(By.XPATH,'//span[@class=\"a-price-whole\"]')\n",
    "for i in price_tags[0:10]:\n",
    "    price = i.text\n",
    "    Car_price.append(price)\n",
    "    \n",
    "\n",
    "    \n",
    "#printing length    \n",
    "print(len(Car_name),len(Car_description),len(Car_price))\n",
    "\n",
    "# making dataframe\n",
    "df = pd.DataFrame({'Car_name':Car_name,'Car_description':Car_description,'Car_price':Car_price})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47fee5c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
